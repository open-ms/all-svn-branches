<!-- $Maintainer: Marc Sturm $ -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator"
        content=
        "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org">
        

  <title>OpenMS Tutorial</title>
  <link rel="stylesheet"
        media="all"
        href="../common/styles.css"
        type="text/css">
</head>

<body>
  <script type="text/javascript"
      src="../common/header.txt">
</script><!-- ........ Content starts here ........ -->


  <h1>Tutorial</h1>
	
	This is a tutorial for people that want to develop new applications using OpenMS.
	<BR>
	If you're not a programmer, have a look at the <a href="../misc/TOPP.html">TOPP</a> section.
	<BR>
	<BR>
	Subsections of this tutorial are:
	<a href="#kernel">Kernel</a> | 
	<a href="#datastructures">Datastructures</a> | 
	<a href="#format">Format</a> | 
	<a href="#metadata">Metadata</a> | 
	<a href="#transformations">Transformations</a> | 
	<a href="#filtering">Filtering</a> | 
	<a href="#visual">Visual</a> |
	<a href="#analysis">Analysis</a>


	<!-- -------------------------------------------------------------------- -->
	<H2>Kernel<a name="kernel"></h2>

  
	<img src="Kernel.png" align=left>  
	
	The <tt>KERNEL</tt> folder contains classes that represent the
  actual MS data: Peaks, Spectra, Features, Maps, ...
	<br clear="all">
	
	<!-- -------------------------------------------------------------------- -->
	<H3>Main datastructures</h3>

  <b>MSSpectrum</b> is a 1-dimensional spectrum of an
  arbitrary peak type that holds meta information as well
  (SpectrumSettings).
  <br>
  <b>MSExperiment</b> is a two dimensional MS
  experiment. It is implemented as a vector of MSSpectrum and is
  derived from ExperimentalSettings for meta information.
	<BR>
	Those two main classes consist of several other classes that are explained now.
	<BR>
	<img src="Kernel_main.png">

 <h3>Traits</h3>

  The traits class used as template argument of many Kernel classes
  determines the type that e.g. position, intensity and charge
  have. By default most of these types are doubles.
  <br>
  If you do not need double precicion, you can use float traits
  instead.

	<!-- -------------------------------------------------------------------- -->
	<H3>D-dimensional coordinates</h3>

  <b>DPosition</b> represents a coordinate in D-dimensional space.
  Dimensions are accessed by <tt>operator[]</tt>.
  <br>
  <b>DimenstionDescription</b> is used in order to have a defined
  order of dimensions, if there is more than one dimension. The
  following example demonstrates its use for a LC-MS experiment:

  <pre>
// typedef to avoid too long name
typedef DimensionDescription &lt; DimensionDescriptionTagLCMS &gt; DimDesc;

// create coordiante
DPosition&lt;2&gt; coordiante;

// sets RT and MZ dimentsion
coordiante[DimDesc::MZ] = 500.0;
coordiante[DimDesc::RT] = 0.1;
</pre><b>DRange</b> is a pair of <tt>DPositions</tt> that spans a
D-dimensions hypercube and offers some convenient methods e.g. for
intersection and center.
</pre>

	<!-- -------------------------------------------------------------------- -->
	<H3>Raw data, peaks and features</h3>

  <b>DRawDataPoint</b> is rawdata point with a D-dimensional
  position and an intensity. Raw data means that no 'peak picking'
  or other pre-processing methods were performed on this data.
  <br>
  <b>DPeak</b> and its derived classes are used for data that on
  which some kind of 'peak picking' was applied (It also adds a
  interface for persistent storage e.g. in a DB). Although
  <tt>DPeak</tt> is a 'picked' peak, it does not contain any
  information about the 'peak picking'.
  <br>
  <b>DPickedPeak</b> is used if more information from any
  pre-processing steps e.g. width or full width at half max, is
  available.
  <br>
  <b>DFeature</b> is also derived from <tt>DPeak</tt> but
  represents a collection of peaks. In general, it summarizes all
  peaks related to a specific peptide or some other chemical
  entity.

	<!-- -------------------------------------------------------------------- -->
	<H3>DSpectrum and peak container</h3>

  <b>DSpectrum</b> represents a D-dimensional spectrum i.e. a
  container of D-dimensional data points (any class derived from
  DRawDataPoint).
  <br>
  One of the template arguments of <tt>DSpectrum</tt> is the
  Container the data is stored in. As container
  <tt>DPeakArray</tt> or a container with the same interface is used.
  <br>
  <b>DPeakArray</b> is vector of data points with a
  more convenient interface for sorting of the data.
  <br>
  <br>
  The following example demonstrates the use of a 2-dimensional
  spectrum of raw data with <tt>DPeakArray</tt> as
  container:

  <pre>
// typedef to avoid too long names
typedef DimensionDescription &lt; DimensionDescriptionTagLCMS &gt; DimDesc;
typedef DSpectrum&lt;2, DPeakArray&lt;2, DRawDataPoint&lt;2&gt; &gt; &gt; Spectrum2D;

// create a spectrum
Spectrum2D spectrum;

// creata a data point
DRawDataPoint&lt;2&gt; data;
data.getPosition()[DimDesc::MZ] = 500.0;
data.getPosition()[DimDesc::RT] = 0.1;
data.getIntensity() = 47.11;

// insert the data point into the spectrum
spectrum.getContainer().push_back(data);

// iterate over the data points
for (Spectrum2D::Iterator it = spectrum.getContainer().begin(); it!=spectrum.getContainer().end(); ++it)
{
  cout &lt;&lt; *it &lt;&lt; endl;
}
</pre>

	<!-- -------------------------------------------------------------------- -->
	<H2>Datastructures<a name="datastructures"></h2>

  The <tt>DATASTRUCTURES</tt> folder contains shared datastructure
  classes, e.g. a convenient string implementation.

	<!-- -------------------------------------------------------------------- -->
	<H2>Format<a name="format"></h2>

  The <tt>FORMAT</tt> folder contains classes for both database and
  file I/O.

	<!-- -------------------------------------------------------------------- -->
	<H3>File IO</h3>

  
  <b>DTAFile</b> is an adapter to read and write DTA files (*.dta).
  DTA is a simple ASCII-based format for MS data. The first line
  contains the singly protonated peptide mass (MH+) and the peptide
  charge state. Subsequent lines contain the fragment ion m/z and
  intensity values.
  <br>
  <br>
  All of the following file adapters share a common interface and
  can read and write (except for <tt>ANDIFile</tt>) to different
  datastructures that implement at least the interface of
  <tt>MSExperiment</tt> with peak type <tt>DRawDataPoint</tt>.
  <br>
  <br>
  <b>DTA2DFile</b> is an adapter to read and write DTA2D files
  (*.dta2d), an extension of DTA to hold LCMS data. Each line
  contains the fragment ion retension time, m/z and intensity
  values.
  <br>
  <b>MzXMLFile</b> is an adapter to read and write mzXML files
  (*.mzXML). Based on XML the format contains MS metadata, e.g.
  instrument settings and sample description, as well as LCMS and
  MSMS spectra in base 64 format. For more information see <a href=
  "http://tools.proteomecenter.org/mzXMLschema.php">Seattle
  Proteome Center</a>.
  <br>
  <b>MzDataFile</b> is an adapter to read and write mzData files
  (*.mzData). Based on XML the format contains MS metadata (conform
  with PSI-OM) as well as LCMS and MSMS spectra in base 64 format.
  For more information see <a href=
  "http://psidev.sourceforge.net/ms/">HUPO PSI-MS</a>.
  <br>
  MzDataFile is able to read and write the additional information
  of picked peaks (<tt>DPickedPeak</tt>) as well.
  <br>
  <b>ANDIFile</b> is an adapter to read ANDI/MS files (*.cdf). The
  binary ANDI/MS format is an <b>an</b>alytical <b>d</b>ata
  <b>i</b>nterchange standard for <b>m</b>ass <b>s</b>pectrometry
  based on netCDF.
  <br>
  <br>
  The following example demonstrates the use of <tt>MzDataFile</tt>
  and <tt>ANDIFile</tt> to convert one format into another using
  <tt>MsExperiment</tt> to hold the data temporarily. The code can
  be used interchangeable with any other file adapter:

  <pre>
// create the file adapters
ANDIFile andi;
MzDataFile mzdata;

// temporary data storage
MSExperiment&lt; DRawDataPoint&lt;1&gt; &gt; map;

// convert ANDI/MS to MzData
andi.load(map,"dummy.cdf");
mzdata.store(map,"dummy.mzData");
</pre>

In order to make the support for different file types easier, the class
<tt>FileHandler</tt> can be used. It loads a file into the appropriate
datastructure independent of the file type.

<pre>
MSExperiment<> in;
FileHandler().loadExperiment("data/MaxReducer_test.dta2d",in);
</pre>


	<!-- -------------------------------------------------------------------- -->
	<H3>Database IO</h3>



  An OpenMS DB can be accessed using the <tt>DBAdapter</tt> class.

	<!-- -------------------------------------------------------------------- -->
	<H2>Metadata<a name="metadata"></h2>

  The <tt>METADATA</tt> folder contains classes for storage of MS
  metadata, e.g. instrument settings and sample description.
  <br>
  <br>
  <b>ExperimentalSettings</b> holds metainformation about the
  experiment the data was created with e.g. the MS instrument, the
  HPLC settings and the operator.
  <br>
  <br>
  <b>SpectrumSettings</b> contains metainformation about settings
  specific to one spectrum, i.e. settings of the MS instrument,
  precorsor information (if it is a MS spectrum) and results of ID
  engines (Mascot, Sequest, ...). <img src="MetaData.png">

	<!-- -------------------------------------------------------------------- -->
	<H3>MetaInfo</h3>

  <img src="MetaInfo.png"
        align="left"> MetaInfo is used to easily store information
        of any type you like, that does not fit into the the other
        classes. It implements Name-Value-Type triplets.
  <br>
  <b>DataValue</b> is a data structure that can store any numerical
  or string information. It also supports casting of the stored
  value back to its original type.
  <br>
  <b>MetaInfo</b> is a associative container that stores
  <tt>DataValues</tt> as values associated to string keys.
  Internally a the string keys are converted to integer keys for
  performance resaons i.e. a <tt>map&lt;UnsignedInt,
  DataValue&gt;</tt> is used.
  <br>
  The <b>MetaInfoRegistry</b> associates the string keys used in
  <tt>MetaValue</tt> with the integer values that are used for
  internal storage. Each key used has to be registered at the
  registry first. The <tt>MetaInfoRegistry</tt> is a singleton.
  <br>
  <br>
  Classes that have one MetaInfo object, inherit from
  <b>MetaInfoInterface</b>.
  <br clear="all">


	<!-- -------------------------------------------------------------------- -->
	<H2>Transformations<a name="transformations"></h2>

  The TRANSFORMATIONS folder contains classes for the
  transformations of raw and peak data.

	<!-- -------------------------------------------------------------------- -->
	<H3>RAW2PEAK</h3>

  This folder contains classes for the transformation from raw to peak data.
  During this so called "peak picking" process we converte the raw ion count data, aquired by the machine, into peak lists
  for the further processing.
  <br>
  <br>
  <b>PeakPickerCWT</b>
  <br>
  This is the main class which you need for peak picking. It uses the multiscale nature of spectrometric data by first detecting the mass peaks in
  the wavelet-transformed signal before a given asymmetric peak function is fitted to  the raw data. 
  In an optional third stage, the resulting fit can be further improved using techniques from nonlinear optimization.
  <br>
  It offers you four different mehtods for peak picking. Two of them work on 
  a container of one-dimensional data points. The following code shows for example how to search for peaks in
  a single MS spectrum:

  <pre>
// A container for the raw data 
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; raw_spectrum;
// Instantiate a file reader for dta files.
DTAFile dta_file;
// Load the raw data stored in "MSSpectrum.dta" into the raw data container.
dta_file.load("MSSpectrum.dta", raw_spectrum);

// instantiate a peak picker object
PeakPickerCWT peak_picker;
// Set the threshold for a minimal peak intensity (height)
peak_picker.setPeakBound(200);
// Set the wavelet scale
peak_picker.setWaveletScale(0.2);
// Enable the optimization of the peak parameters
peak_picker.setOptimizationFlag(true);
// Enable the deconvolution of overlapping peaks
peak_picker.setDeconvolutionFlag(true);
      
// Container for the resulting peaks
MSSpectrum &lt;DPickedPeak &lt;1&gt; &gt; peak_spectrum;

// First possibility: pick the peaks on a well defined iterator range
peak_picker.pick(raw_spectrum.begin(), raw_spectrum.end(), peak_spectrum); 

// Second possibility: pick the peaks on the whole raw data container
peak_picker.pick(raw_spectrum, peak_spectrum);
</pre>

The other two methods work on containers of MSSpectra (e.g. vector<MSSpectrum<DRawDataPoint<1> > >). 
The following code example should explain the usage:

 <pre>
// Instantiate a MSExperiment for the raw data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; raw_experiment;
// Instantiate a file reader for mzData files.
MzDataFile mzdata_file;
// Load the raw data stored in "MSExperiment.mzData" into the raw_experiment
mzdata_file.load("MSExperiment.mzData", raw_experiment);

// instantiate a peak picker object
PeakPickerCWT peak_picker;
// Set the threshold for a minimal peak intensity (height)
peak_picker.setPeakBound(200);
// Set the threshold for a minimal peak intensity (height) in tandem mass spectra
peak_picker.setPeakBoundMs2Level(50);
// Set the wavelet scale
peak_picker.setWaveletScale(0.2);
// Enable the optimization of the peak parameters
peak_picker.setOptimizationFlag(true);

// Container for the resulting peaks
MSExperiment &lt;DPickedPeak &lt;1&gt; &gt; peak_experiment;

// First possibility: pick the peaks on a well defined iterator range 
// (note: this method does not copy the experimental settings of the raw data)
peak_picker.pickExperiment(raw_experiment.begin(), raw_experiment.end(), peak_experiment); 

// Second possibility: pick the peaks on the whole raw_experiment
peak_picker.pickExperiment(raw_experiment, peak_experiment);
</pre>

Instead of setting the peak picking parameter manually you can also use a parameter file.
The constructor of the peak picker needs either the peak
picking parameter stored in a xml file or in a Param object (when
the Param object or the file are empty the default peak picking
paramter are used). Some sample xml files can be found in
OpenMS/source/TRANSFORMATIONS/RAW2PEAK.
  <br>
  <br>

  <b>Note:</b>

  <ul>
    The Optimization of the peak parameter turned out to be
    particularly useful for poorly resolved data with strongly
    overlapping, convoluted peak patterns!</ul>

  <br>
  <br>
  <b>ContinuousWaveletTransform</b>
  <br>
  The DPeakPickerCWT uses the continiuous wavelet transformation
  (CWT) of the raw data to detect potential peaks.
  <br>
  <br>
  <b>ContinuousWaveletTransformNumIntegration</b>
  <br>
  This class computes the convolution happens in the CWT by
  numerical integration.
  <br>
  <br>
  
  <b>DSignalToNoiseEstimator</b>
  <br>
  This class provides the estimation of the signal to noise ratios
  in a given raw data points intervall.
  <br>
  <br>
  <b>DSignalToNoiseEstimatorWindowing</b>
  <br>
  This class computes the signal to noise ratio using the method of
  Roegnvaldsson et al. described in "Modular, Scriptable, and
  Automated Analysis Tools for High-Throughput Peptide Mass
  Fingerprinting".
  <br>
  <br>
  <b>OptimizePick</b>
  <br>
  This class implements the optional peak parameter optimization.
  In the previous stage, each of the peaks has been fitted
  independently of the others, but for a true separation, we need
  to fit the sum of all peaks to the experimental signal.
  OptimizePeak offers a standard technique from nonlinear
  optimization, the Levenberg-Marquardt algorithm.
  <br>
  <br>
  <b>OptimizePeakDeconvolution</b>
  <br>
  This class implements the optional deconvolution of overlapping
  peaks. If a peak is too broad or asymmetric it is assumed that there
  are more underlying peaks. They are detected using the CWT.
  Afterwards the peak parameters of the "new" peaks are optimized
  using the Levenberg-Marquardt algorithm. Here again the sum of the
  peaks is fitted to the experimental signal.
  <br>
  <br>
  <b>TwoDOptimization</b>
  <br>
  This class first detects peak clusters, i.e. peaks that occur
  shortly after one another. Afterwards the peak parameters in the
  clusters are optimized. Here one can choose between applying
  OptimizePick for each scan in the cluster (which yields a speed
  improvement compared to applying OptimizePick to the whole scan)
  or a two dimensional optimization. Here peaks occuring in several
  scans get the same m/z-value.


	
	<!-- -------------------------------------------------------------------- -->
	<H3>Feature Finder</h3>

  The FeatureFinder implements an algorithms for the identification
  of features in LC/MS maps. By feature, we understand a chemical
  compound, usually a peptide, contained in the sample. Our
  algorithm is based on an approach that was published in <i>Groepl
  et al. (2005) Proc. CompLife-05</i> . Essentially, we identify
  data points with a high intensity (<i>seeding</i>), collect a
  region of data points around each seed (<i>extension</i>) and fit
  a two-dimensional model to each region (<i>model fitting</i>).
  This model consists of gaussian mixture representing the m/z
  dimension and a bigaussian distribution that is fitted in rt
  dimension. If the quality of this fit is sufficiently high, we
  declare this region as feature, estimate its charge and
  coordinates and continue with the next seed.
  <br>
  The FeatureFinder application consists of four different modules:
  a <b>traits</b> class, <b>seeder</b>, <b>extender</b> and
  <b>model fitter</b>. The traits class reads the data from a file,
  estimates the background noise level and provides access to
  intensity and coordinates of each data point to the individual
  modules. These modules implements the three steps in the feature
  finding algoririthm, as described above. Each module has several
  parameters. For a short description of each module and its
  parameters, please have a look at the corresponding header file
  in the directory
  <tt>/include/TRANSFORMATIONS/FEATUREFINDER/</tt> or at the Doxygen
  documentation of OpenMS. Please note that the standard settings of
  this algorithm apply to well-resolved data sets of reasonable size.
  In some situations good results will require some manual fine-tuning
  of the parameters.
  <br>
  There are two possibilities to use the feature finding
  algorithms. The first and easiest one is to use the corresponding
  TOPP (TOPP = The OpenMS Proteomics Pipeline) module located in
  <tt>source/APPLICATIONS/TOPP/</tt>. It is a commandline program
  that offers an online help and is pretty self-explanatory.
  <br>
  The second possibility, for the more advanced user who might even
  want to implement his or her own modules, is to use OpenMS as a
  library and write an FeatureFinder application of your own. This
  is straightforward to do. A short example is given below:

  <pre>
DTA2DFile dta2d_file;
MSExperiment exp;

dta2d_file.load("path_to_rawdata_file",exp);

FeatureFinder ff;
ff.setData(exp);
DfeatureMap&lt;2&gt; features;
features = ff.run();
</pre>
  <br>
  This small program loads a data set from a dta2d file, loads this
  data into the FeatureFinder class and runs the algorithm. The
  resulting features are stored in an instance of class DFeatureMap
  which is a simple container for features. The feature map can be
  stored or used for further processing.
  <br>
  The design of the FeatureFinder is in fact a bit more complex
  that outlined here. For details please have a look at the class
  diagram given below. <img src="FeatureFinder.png">

	<!-- -------------------------------------------------------------------- -->
	<H2>Filtering<a name="filtering"></h2>

  The FILTERING folder contains classes for calibration, smoothing
  and the baseline-filtering of raw data as well as a class for 
  filtering identifications.

	<!-- -------------------------------------------------------------------- -->
	<H3>Baseline filtering</h3>

  This is folder contains classes for baseline filtering of raw
  data.
  <br>
  <br>
  <b>TopHatFilter</b>
  <br>
  This class implements a morphological baseline filter. The filter will remove all structures 
  in the signal which are broader than a given the structuring element.
 <br>
  It offers you four different mehtods to invoke the baseline filtering algorithm. Two of them work on 
  a container of one-dimensional data points. The following code shows for example how to filter 
  a single MS spectrum:

  <pre>
// A container for the raw data 
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; raw_spectrum;
// Instantiate a file reader for dta files.
DTAFile dta_file;
// Load the raw data stored in "MSSpectrum.dta" into the raw data container.
dta_file.load("MSSpectrum.dta", raw_spectrum);

// instantiate a top hat filter object
TophatFilter tophat;
// Set the width of the structuring element (given in Thomson)
tophat.setStrucElemSize(5);

// Container for the resulting filtered data
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; filtered_spectrum;

// First possibility: filter the raw data in a well defined iterator range
tophat.filter(raw_spectrum.begin(), raw_spectrum.end(), filtered_spectrum); 

// Second possibility: filter the whole raw data container
tophat.filter(raw_spectrum, filtered_spectrum);
</pre>

The other two methods work on containers of MSSpectra (e.g. vector<MSSpectrum<DRawDataPoint<1> > >). 
The following code example should explain the usage:

 <pre>
// Instantiate a MSExperiment (a container for raw data containers) for the raw data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; raw_experiment;
// Instantiate a file reader for mzData files.
MzDataFile mzdata_file;
// Load the raw data stored in "MSExperiment.mzData" into the raw_experiment
mzdata_file.load("MSExperiment.mzData", raw_experiment);

// instantiate a top hat filter object
TophatFilter tophat;
// Set the width of the structuring element (given in Thomson)
tophat.setStrucElemSize(5);

// Container for the resulting filtered data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; filtered_experiment;

// First possibility: filter the raw data in a well defined iterator range 
// (note: this method does not copy the experimental settings of the raw data)
tophat.filterExperiment(raw_experiment.begin(), raw_experiment.end(), filtered_experiment);

// Second possibility: filter the whole raw_experiment
tophat.filterExperiment(raw_experiment, filtered_experiment);
</pre>

Instead of setting the width of the structuring element manually you can also use a parameter file.
The two other constructors of the top hat filter need either the parameters stored in a xml file or in a Param object.
  
	<!-- -------------------------------------------------------------------- -->
	<H3>Smoothing</h3>

  This folder contains smoothing filter classes for raw data.
  <br>
  <br>
  <b>GaussFilter</b>
  <br>
  This class represents a Gaussian lowpass-filter.
  <br>
  <br>
  It offers you four different mehtods to invoke the gaussian filtering algorithm. Two of them work on 
  a container of one-dimensional data points. The following code shows for example how to filter 
  a single MS spectrum:

  <pre>
// A container for the raw data 
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; raw_spectrum;
// Instantiate a file reader for dta files.
DTAFile dta_file;
// Load the raw data stored in "MSSpectrum.dta" into the raw data container.
dta_file.load("MSSpectrum.dta", raw_spectrum);

// Instantiate a gaussian filter object
GaussFilter gaussian;
// Set the standard deviation of the filter and the spacing of the filter coefficients 
gaussian.init(0.1,0.001);

// Container for the resulting filtered data
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; filtered_spectrum;

// First possibility: filter the raw data in a well defined iterator range
gaussian.filter(raw_spectrum.begin(), raw_spectrum.end(), filtered_spectrum); 

// Second possibility: filter the whole raw data container
gaussian.filter(raw_spectrum, filtered_spectrum);
</pre>

The other two methods work on containers of MSSpectra (e.g. vector<MSSpectrum<DRawDataPoint<1> > >). 
The following code example should explain the usage:

 <pre>
// Instantiate a MSExperiment (a container for raw data containers) for the raw data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; raw_experiment;
// Instantiate a file reader for mzData files.
MzDataFile mzdata_file;
// Load the raw data stored in "MSExperiment.mzData" into the MSExperiment
mzdata_file.load("MSExperiment.mzData", raw_experiment);

// instantiate a top hat filter object
GaussFilter tophat;
// Set the standard deviation of the filter and the spacing of the filter coefficients 
gaussian.init(0.1,0.001);

// Container for the resulting filtered data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; filtered_experiment;

// First possibility: filter the raw data in a well defined iterator range 
// (note: this method does not copy the experimental settings of the raw data)
gaussian.filterExperiment(raw_experiment.begin(), raw_experiment.end(), filtered_experiment);

// Second possibility: filter the whole raw MSExperiment
gaussian.filterExperiment(raw_experiment, filtered_experiment);
</pre>
  <br>
  <b>Note:</b> The gaussian filter is normalized, so the area under
  a peak is preserved, but the maximum position gets blurred.
  <br>
  <br>
  <b>SavitzkyGolayQRFilter and SavitzkyGolaySVDFilter</b>
  <br>
  This classes represent a SavitzkyGolay lowpass-filter.
<br>

 <br>
  Both classes offers the same interface. You can use four different mehtods to invoke the gaussian filtering algorithm. Two of them work on 
  a container of one-dimensional data points. The following code shows for example how to filter 
  a single MS spectrum:

  <pre>
// A container for the raw data 
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; raw_spectrum;
// Instantiate a file reader for dta files.
DTAFile dta_file;
// Load the raw data stored in "MSSpectrum.dta" into the raw data container.
dta_file.load("MSSpectrum.dta", raw_spectrum);

// Instantiate a gaussian filter object
SavitzkyGolaySVDFilter sgolay;
// Set the size of the filter kernel (given by the number of data points!)
sgolay.setWindowSize(17);
// Set the order of the smoothing polynom
sgolay.setOrder(4);

// Container for the resulting filtered data
MSSpectrum &lt;DRawDataPoint &lt;1&gt; &gt; filtered_spectrum;

// First possibility: filter the raw data in a well defined iterator range
sgolay.filter(raw_spectrum.begin(), raw_spectrum.end(), filtered_spectrum); 

// Second possibility: filter the whole raw data container
sgolay.filter(raw_spectrum, filtered_spectrum);
</pre>

The other two methods work on containers of MSSpectra (e.g. vector<MSSpectrum<DRawDataPoint<1> > >). 
The following code example should explain the usage:

 <pre>
// Instantiate a MSExperiment (a container for raw data containers) for the raw data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; raw_experiment;
// Instantiate a file reader for mzData files.
MzDataFile mzdata_file;
// Load the raw data stored in "MSExperiment.mzData" into the raw_experiment
mzdata_file.load("MSExperiment.mzData", raw_experiment);

// Instantiate a gaussian filter object
SavitzkyGolaySVDFilter sgolay;
// Set the size of the filter kernel (given by the number of data points!)
sgolay.setWindowSize(17);
// Set the order of the smoothing polynom
sgolay.setOrder(4);

// Container for the resulting filtered data
MSExperiment &lt;DRawDataPoint &lt;1&gt; &gt; filtered_experiment;

// First possibility: filter the raw data in a well defined iterator range 
// (note: this method does not copy the experimental settings of the raw data)
sgolay.filterExperiment(raw_experiment.begin(), raw_experiment.end(), filtered_experiment);

// Second possibility: filter the whole raw_experiment
sgolay.filterExperiment(raw_experiment, filtered_experiment);
</pre>
  <br>
  <b>Note:</b>

  <ul>
    <li>By decreasing the order of the polynomial or increasing the
    number of filter coefficients, the filtered signal gets smoother (the
    order x should be 2 &lt;= x &lt;= 4) !</li>

    <li>The Savitzky Golay filter preserves the maximum position of
    a peak much better than the Gaussian filter!</li>

    <li>The SavitzkyGolayQRFilter and the SavitzkyGolaySVDFilter
    generate the same filtering, but they vary in run time. Try
    which one is more suitable for your application!</li>
  </ul>


	<!-- -------------------------------------------------------------------- -->
	<H2>Visual<a name="visual"></h2>

  The <tt>VISUAL</tt> folder contains classes of the GUI.


	<!-- -------------------------------------------------------------------- -->
	<H2>Analysis<a name="analysis"></h2>

  The ANALYSIS folder contains algorithms and modules for the high-level processing and analysis of LC/MS data.
  
	<!-- -------------------------------------------------------------------- -->
	<H3>MAPMATCHING</h3>

  <br>
  The MAPMATHING folder contains classes for <b>Map Alignment</b>.
  During Map Alignment, we try to identify corresponding elements in multiple maps.
  This elements can be of type DFeature, DPeak, ConsensusFeature or ConsensusElement.
  To compute the multiple alignment (that is given by a final consensus map), a reference element map
  is chosen and all other maps are aligned pairwise to the refernce map.
  The pairwise alignment is done by classes inherited from BasePairwiseMapMatcher, like the 
  PoseClusteringPairwiseMapMatcher. 
  In a first step the PoseClusteringPairwiseMapMatcher uses a superimposer class to estimate 
  the transformation between the element positions of the two element maps.
  Given this first estimate of the transformation, reliable element pairs can be selected using a pair finder class.
  These element pairs serve as a kind of landmarks to estimate (e.g. by a linear regression) again a second more 
  reliable transformation that maps the positions of one map onto the other.
  In the second step another pair finder is used to compute the consensus elements in both maps and stores them
  in a final consensus map.
  The following code shows how to load multiple feature maps, aligns them and stores the resulting consensus map can be stored in ConsensusXML format. 
   <pre> 
// Vector containing the file names of three feature maps 
std::vector&lt; String > file_names(3);
// 
file_names[0] = "FeatureMap1.xml";
file_names[1] = "FeatureMap2.xml";
file_names[2] = "FeatureMap3.xml";

// Vector for the feature maps
std::vector&lt; FeatureMap &gt; feature_maps(3);

// FeatureXML file handler
DFeatureMapFile feature_file;
feature_file.load(file_names[0],feature_maps[0]);
feature_file.load(file_names[1],feature_maps[1]);
feature_file.load(file_names[2],feature_maps[2]);

// Initialite a StarAlignment object (that uses default parameters)
StarAlignment&lt; ConsensusFeature&lt; FeatureMap &gt; &gt; alignment;

// Set the type of the element maps 
alignment.setMapType("feature_map");

// The alignment needs the references to the feature map
alignment.getElementMapVector().push_back(&#38(feature_maps[0]));
alignment.getElementMapVector().push_back(&#38(feature_maps[1]));
alignment.getElementMapVector().push_back(&#38(feature_maps[2]));

// as well as the file names      
alignment.setFileNames(file_names);

// Start the alignment  
alignment.run();

// and store the result in ConsensusXML format      
ConsensusXMLFile cons_file;
cons_file.store("Consensus.xml",alignment);
</pre>
  <img src="MapAlignment.png" align=left>  
</body>
</html>

<!--<h3>Map Matching</h3>

  After a raw data map has been picked and searched for features,
  we need to map features from different maps i.e. experiments onto
  each other before we are able to make meaningful comparisons.
  That means that we want to identify groups of features that stem
  from the same peptide in different experiments. In OpenMS, this
  procedure consists of three steps: <b>feature matching</b>,
  <b>map mapping</b> and <b>map dewarping</b>.
  <br>
  <br>
  <b>Feature Matching</b>
  <br>
  In this step, we try to identify pairs of features in different
  maps. So far, two different approaches have been implemented. The
  first and simpler one is a clustering-like approach and tries to
  cluster features to pairs based. The distance is a function of
  retention time, m/z and intensity of the features. The second
  approach is based on <i>Geometric Hashing</i>. It will usually
  work better than the simple approach if there is a significant
  shift in rt and/or m/z between different experiments.
  <br>
  <br>
  <b>Map Mapping</b>
  <br>
  The map mapping step tries to estimate a mapping function that
  maps the coordinates of the feature partners that have been
  identified in the last step on each other. Currently we estimate
  a piecewise linear function using linear regression but it is
  easy to extend our method to more sophisticated approaches.
  <br>
  <br>
  <b>Map Dewarping</b>
  <br>
  The last step in this workflow is called dewarping. It merely
  consists of the application of the mapping function that was
  estimated in the last step to the feature coordinates. After the
  dewarping, features that have been identified as partners should
  now have very similar if not identical coordinates. This allows
  an easy comparison of corresponding features in different maps.
  <br>
  <br>
  The directory <tt>source/EXAMPLES</tt> contains an example
  program which shows how to use the diferent classes and modules
  in the map mapping workflow. -->
